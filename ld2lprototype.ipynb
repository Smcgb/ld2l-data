{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LD2L data project\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In order to pull interesting data from ld2l.gg I have created a basic set of pulls to gather match data and parse it into a dataframe. \n",
    "This dataframe is exported to a csv and can be used with any BI software or can be read programatiicaly with dataframe packages for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import time\n",
    "import os\n",
    "\n",
    "# You will need your own API key from opendota \n",
    "# and a config.py file that sets the variable api_key to your key\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling season info\n",
    "\n",
    "Basic match data is found on ld2l.gg/seasons/##/matches. My outline here will use season 37 for prototyping. \n",
    "\n",
    "Using BeautifulSoup (BS) (https://www.crummy.com/software/BeautifulSoup/bs4/doc/) the ld2l matches page is parsed to get the ld2l match id. \n",
    "The ld2l match id does not match the dota match id\n",
    "\n",
    "A cache file is created, unless it already exists, to avoid re-parsing saved info.\n",
    "\n",
    "Note that seasons on the website do not relate to the ticket or season that would be listed in Dota/OpenDota api.\n",
    "This method also reduces adding in matches that ticket holders use for scrims or other reasons that aren't official games.\n",
    "One limitation is that this method can't pull unticketed data, even if entered in completly on the ld2l website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set ld2l season webpage\n",
    "url = 'https://ld2l.gg/seasons/13/matches'\n",
    "season = url.split('/')[-2]\n",
    "\n",
    "soup = bs4.BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "matches = []\n",
    "\n",
    "# This is a directory string to help manage the data of different seasons\n",
    "save_dir = f'match_data_{url.split(\"/\")[-2]}'\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if 'match' in a['href'] and 'season' not in a['href']:\n",
    "        matches.append('https://ld2l.gg' + a['href'])\n",
    "\n",
    "#sort matches by ID\n",
    "matches.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "# create a folder to store match data if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "# matches text file location to variable\n",
    "matches_file = f'{save_dir}/matches_{season}.txt'\n",
    "\n",
    "# create a matches text file to store match IDs if it doesn't exist\n",
    "\n",
    "if not os.path.exists(matches_file):\n",
    "    with open(matches_file, 'w') as f:\n",
    "        f.write('')\n",
    "\n",
    "len(matches)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to OpenDota links\n",
    "\n",
    "After gathering the match data, each match page is opened via BS. From here the Match ID is extracted from the \n",
    "OpenDota link and a correctly formatted OpenDota API link is added to a list.\n",
    "\n",
    "This section skips over matches that have been parsed already by checking the matches.txt file created in the last secion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below code is for getting opendota links\n",
    "\n",
    "od_matches = []\n",
    "\n",
    "for match in matches:\n",
    "    #check if match is already in file matches.txt to prevent re-scraping and angry butterygreg\n",
    "    if match in open(matches_file).read():\n",
    "        pass\n",
    "    else:\n",
    "        #write match to file\n",
    "        with open(matches_file, 'a') as f:\n",
    "            f.write(match + '\\n')\n",
    "\n",
    "\n",
    "        soup = bs4.BeautifulSoup(requests.get(match).text, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'opendota' in a['href']:\n",
    "                if 'matches/0' in a['href']:\n",
    "                    break\n",
    "            # get match id from end of url\n",
    "                match_id = a['href'].split('/')[-1]\n",
    "\n",
    "                # here an opendota link is created and appended to the list, note the api key is required\n",
    "                od_matches.append(f\"https://api.opendota.com/api/matches/{match_id}?api_key={api_key}\")\n",
    "                break\n",
    "\n",
    "len(od_matches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling OpenDota Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold list of file names\n",
    "file_names = []\n",
    "\n",
    "for files in os.listdir(save_dir):\n",
    "    if files.endswith('.json'):\n",
    "        file_names.append(files)\n",
    "\n",
    "for i, match in enumerate(od_matches):\n",
    "\n",
    "    file_path = f'{save_dir}/match_{match.split(\"/\")[-1]}.json'\n",
    "\n",
    "    # get match id\n",
    "    match_id = match.split('/')[-1].split('?')[0]\n",
    "\n",
    "    #check if file already exists\n",
    "    if os.path.isfile(f'{file_path}'):\n",
    "        pass\n",
    "    else:\n",
    "        if match_id == '0':\n",
    "            pass\n",
    "        # get json of match and save to json file\n",
    "        else:\n",
    "            match_json = requests.get(match).json()\n",
    "            with open(f'{save_dir}/match_{match_id}.json', 'w') as f:\n",
    "                json.dump(match_json, f)\n",
    "                file_names.append(f'{save_dir}/match_{match_id}.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame formatting and basic cleaning\n",
    "\n",
    "Below a blank dataframe is created with the selected features from the players section in the read json files. \n",
    "As with earlier sections, if a cached match_data.csv exists, new items will be concatenated instead of a new creations, saving time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold all match data if it doesn't exist\n",
    "\n",
    "if not os.path.exists('match_data.csv'):\n",
    "    match_data = pd.DataFrame(columns=['match_id', 'date', 'week', 'account_id', 'personaname', 'teamID', 'rank_tier', 'kills', 'assists',\n",
    "       'deaths', 'kills_per_min', 'kda', 'denies', 'gold', 'gold_per_min', 'gold_spent', 'hero_damage', 'damage_taken',\n",
    "       'hero_healing', 'hero_id', 'item_0', 'item_1', 'item_2', 'item_3',\n",
    "       'item_4', 'item_5', 'item_neutral', 'last_hits', 'level',\n",
    "       'net_worth', 'tower_damage', 'xp_per_min', 'radiant_win',\n",
    "       'duration', 'patch', 'isRadiant', 'win', 'lose',\n",
    "       'total_gold', 'total_xp', 'obs_placed', 'sen_placed', 'rune_pickups', 'camps_stacked', 'stuns', 'creeps_stacked',\n",
    "       'firstblood_claimed', 'pings', 'teamfight_participation', 'roshans_killed'])\n",
    "    match_data.to_csv('match_data.csv')\n",
    "else:\n",
    "    match_data = pd.read_csv('match_data.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  i, file in enumerate(file_names):\n",
    "\n",
    "    # read first json file as a dictionary\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # get match id\n",
    "    match_id = data['match_id']\n",
    "\n",
    "    # if match id is already in matches_df, skip\n",
    "    if match_id in match_data['match_id'].values:\n",
    "        pass\n",
    "    else:\n",
    "\n",
    "        rad_team_id = data['radiant_team_id']\n",
    "        dire_team_id = data['dire_team_id']\n",
    "        \n",
    "    # read player from data into a dataframe\n",
    "\n",
    "        df = pd.DataFrame(data['players'])\n",
    "\n",
    "        # damage taken needs to be transformed. it is a nested dictionary and should be replaced with the sum of the values\n",
    "\n",
    "        df['damage_taken'] = df['damage_taken'].apply(lambda x: sum(x.values()))\n",
    "\n",
    "        #convert start_time from unix time to datetime using\n",
    "        df['start_time'] = pd.to_datetime(df['start_time'], unit='s')\n",
    "        df['date'] = df['start_time'].dt.date\n",
    "\n",
    "        #games are played weekly. create a column for the week of the game. Week 1 starts on 2023-01-22, using isocalendar\n",
    "        df['week'] = df['start_time'].dt.isocalendar().week - 2\n",
    "\n",
    "        #drop start_time\n",
    "        df.drop('start_time', axis=1, inplace=True)\n",
    "\n",
    "        # if isRadiant is true, set teamID to radiant team ID, else set to dire team ID\n",
    "\n",
    "        df['teamID'] = df['isRadiant'].apply(lambda x: rad_team_id if x == True else dire_team_id)\n",
    "\n",
    "        new_order = match_data.columns.tolist()\n",
    "\n",
    "        df = df[new_order]\n",
    "\n",
    "        # append to main df via concat\n",
    "\n",
    "        match_data = pd.concat([match_data, df], axis=0)\n",
    "\n",
    "        # replace NaN with 0\n",
    "        match_data.fillna(0, inplace=True)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # save to csv every loop\n",
    "        match_data.to_csv('match_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview\n",
    "\n",
    "Below will be a dataframe preview. Please note that some assumptions may cause the wrong week to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototype for function to get match data from opendota, will test on a different season than the prototype\n",
    "\n",
    "def get_ld2l_matches(season):\n",
    "    \"\"\"\n",
    "    This function reads ld2l.gg to get the specific match IDs that are ticketed and registered on OpenDota into a list. \n",
    "    This list is saved as a text file to prevent re-scraping of data.\n",
    "\n",
    "    This function also creates a folder to store the match data in and will be the directory for the matches.txt file and later the match data.\n",
    "\n",
    "    inputs: season (int) - the season of the matches to be pulled as listed on ld2l.gg matches page, different than the opendota season due to ticketing issues\n",
    "    Returns: \n",
    "        list of ld2l.gg match IDs, \n",
    "        save_dir (str) - directory string to help manage the data of different seasons to pass to the next function\n",
    "        matches.txt directory for future use in following functions\n",
    "\n",
    "    output: text file that matches.txt that another function will use to check if the match data has already been pulled and write to.\n",
    "    \"\"\"\n",
    "    url = f'https://ld2l.gg/seasons/{season}/matches'\n",
    "    soup = bs4.BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    matches = []\n",
    "\n",
    "    # This is a directory string to help manage the data of different seasons\n",
    "    save_dir = f'match_data_{url.split(\"/\")[-2]}'\n",
    "\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if 'match' in a['href'] and 'season' not in a['href']:\n",
    "            matches.append('https://ld2l.gg' + a['href'])\n",
    "\n",
    "    #sort matches by ID\n",
    "    matches.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n",
    "    # create a folder to store match data if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    # save match file location to a text file\n",
    "\n",
    "    matches_file = f\"{save_dir}/matches_{season}.txt\"\n",
    "\n",
    "    # create a matches text file to store match IDs if it doesn't exist\n",
    "    if not os.path.exists(f'{matches_file}'):\n",
    "        with open(f'{matches_file}', 'w') as f:\n",
    "            f.write('')\n",
    "\n",
    "    return matches, save_dir, matches_file\n",
    "\n",
    "def get_od_matches(matches, save_dir, matches_file):\n",
    "    \"\"\"\n",
    "    This function takes the list of ld2l.gg match IDs and converts them to opendota match IDs.\n",
    "\n",
    "    input: \n",
    "        list of ld2l.gg match IDs, ideally from the get_ld2l_matches function \n",
    "        the save_dir string from that function\n",
    "        the matches_file string from that function\n",
    "\n",
    "    Returns: \n",
    "        list of opendota match IDs, \n",
    "        save_dir (str) - the directory string for the match data to pass to the next function\n",
    "\n",
    "    output: writes match IDs to matches.txt file to prevent re-scraping of data\n",
    "    \"\"\"\n",
    "    \n",
    "    od_matches = []\n",
    "\n",
    "    for match in matches:\n",
    "        #check if match is already in matches.txt\n",
    "        if match in open(matches_file).read():\n",
    "            pass\n",
    "        else:\n",
    "            #write to matches file\n",
    "            with open(matches_file, 'a') as f:\n",
    "                f.write(match + '\\n')\n",
    "\n",
    "            # reads html from ld2l.gg match page and finds the opendota match ID\n",
    "            soup = bs4.BeautifulSoup(requests.get(match).text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                if 'opendota' in a['href']:\n",
    "                    # if match id is 0, it means the match wasnt ticketed and can be ignored\n",
    "                    if \"matches/0\" in a['href']:\n",
    "                        break\n",
    "                    else:\n",
    "                        match_id = a['href'].split('/')[-1]\n",
    "                        od_matches.append(f'https://api.opendota.com/api/matches/{match_id}')\n",
    "                \n",
    "    return od_matches, save_dir\n",
    "\n",
    "def get_match_data(od_matches, save_dir):\n",
    "    \"\"\"\n",
    "    This function pulls the raw json data for each match and saves as a json file in the save_dir folder.\n",
    "\n",
    "    Inputs:\n",
    "        list of opendota match IDs, ideally from the get_od_matches function\n",
    "        the save_dir string from that function\n",
    "\n",
    "    Returns: save_dir (str) - the directory string for the match data to pass to the next function\n",
    "    \"\"\"\n",
    "\n",
    "    file_names = []\n",
    "\n",
    "    for files in os.listdir(save_dir):\n",
    "        if files.endswith('.json'):\n",
    "            file_names.append(files)\n",
    "\n",
    "    for i, match in enumerate(od_matches):\n",
    "\n",
    "        file_path = f'{save_dir}/match_{match.split(\"/\")[-1]}.json'\n",
    "\n",
    "        # get match id\n",
    "        match_id = match.split('/')[-1].split('?')[0]\n",
    "\n",
    "        #check if file already exists\n",
    "        if os.path.isfile(f'{file_path}'):\n",
    "            pass\n",
    "        else:\n",
    "            if match_id == '0':\n",
    "                pass\n",
    "            # get json of match and save to json file\n",
    "            else:\n",
    "                match_json = requests.get(match).json()\n",
    "                with open(f'{save_dir}/match_{match_id}.json', 'w') as f:\n",
    "                    json.dump(match_json, f)\n",
    "                    file_names.append(f'{save_dir}/match_{match_id}.json')\n",
    "\n",
    "    return file_names, save_dir\n",
    "\n",
    "def create_season_dataframe(file_names, save_dir):\n",
    "    \"\"\"\n",
    "    This function reads the json files in the save_dir folder and creates a dataframe of the match data.\n",
    "    The dataframe is saved as a csv file for processing outside of this script. \n",
    "\n",
    "    If the csv file already exists, it will be read and returned with new data appended to the end.\n",
    "\n",
    "    Inputs: \n",
    "        file_names (list) - list of file names in the save_dir folder\n",
    "        save_dir (str) - the directory string for the match data. Ideally passed from previous function.\n",
    "    Returns: match_data (pd.DataFrame) - the dataframe of match data\n",
    "    Output: match_data.csv file in save_dir folder\n",
    "    \"\"\"\n",
    "\n",
    "    # create an empty dataframe to hold all match data if it doesn't exist in format\n",
    "    # match_data_{season}.csv\n",
    "\n",
    "    # csv filepath to variable\n",
    "\n",
    "    csv_path = f'{save_dir}/match_data_{save_dir.split(\"_\")[-1]}.csv'\n",
    "\n",
    "    if not os.path.exists(f'{csv_path}'):\n",
    "        match_data = pd.DataFrame(columns=[\n",
    "            'match_id', 'date', 'week', 'account_id', 'personaname', 'teamID', \n",
    "            'rank_tier', 'kills', 'assists''deaths', 'kills_per_min', 'kda', \n",
    "            'denies', 'gold', 'gold_per_min', 'gold_spent', 'hero_damage', 'damage_taken',\n",
    "            'hero_healing', 'hero_id', 'item_0', 'item_1', 'item_2', 'item_3',\n",
    "            'item_4', 'item_5', 'item_neutral', 'last_hits', 'level',\n",
    "            'net_worth', 'tower_damage', 'xp_per_min', 'radiant_win', \n",
    "            'duration', 'patch', 'isRadiant', 'win', 'lose', 'total_gold', \n",
    "            'total_xp', 'obs_placed', 'sen_placed', 'rune_pickups', 'camps_stacked', \n",
    "            'stuns', 'creeps_stacked', 'firstblood_claimed', 'pings', 'teamfight_participation', \n",
    "            'roshans_killed']\n",
    "        )\n",
    "        match_data.to_csv(f'{csv_path}', index=False)\n",
    "    else:\n",
    "        match_data = pd.read_csv(f'{csv_path}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # read json files in save_dir and append to match_data\n",
    "\n",
    "    for i, file in enumerate(file_names):\n",
    "\n",
    "        # read json file\n",
    "        with open(file) as f:\n",
    "            match = json.load(f)\n",
    "        \n",
    "        # get match id\n",
    "        match_id = match['match_id']\n",
    "\n",
    "        # check if match is already in match_data, passes if parsed already\n",
    "        if match_id in match_data['match_id'].values:\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            # get team ids\n",
    "            radiant_team_id = match['radiant_team']['team_id']\n",
    "            dire_team_id = match['dire_team']['team_id']\n",
    "\n",
    "            # read player data into a temporary dataframe\n",
    "            # the dataframe will be concatenated to the match_data dataframe\n",
    "\n",
    "            df = pd.DataFrame(data['players'])\n",
    "\n",
    "            #flattening the damage_taken feater\n",
    "\n",
    "            df['damage_taken'] = df['damage_taken'].apply(lambda x: sum(x.values()))\n",
    "\n",
    "            # convert unix time to datetime\n",
    "\n",
    "            df['start_time'] = pd.to_datetime(df['start_time'], unit='s')\n",
    "            df['date'] = df['start_time'].dt.date\n",
    "\n",
    "            # games are played weekly. create a column for the week of the game. \n",
    "            # This will need to be transformed to the correct week number for each season\n",
    "            # If games are played early or late in the week, this will need to be adjusted\n",
    "            # It will also not work for seasons that go over a year boundary\n",
    "            df['week'] = df['start_time'].dt.isocalendar().week\n",
    "\n",
    "            # drop unix start time\n",
    "            df.drop('start_time', axis=1, inplace=True)\n",
    "\n",
    "            # assign team id to each player\n",
    "\n",
    "            df['teamID'] = df['isRadiant'].apply(lambda x: rad_team_id if x == True else dire_team_id)\n",
    "\n",
    "            # make columns the same as the match_data dataframe\n",
    "\n",
    "            new_order = match_data.columns\n",
    "\n",
    "            df = df[new_order]\n",
    "\n",
    "            # append to match_data\n",
    "\n",
    "            match_data = match_data.append(df, ignore_index=True)\n",
    "\n",
    "            # fill in missing values\n",
    "            # replace NaN with 0\n",
    "            match_data.fillna(0, inplace=True)\n",
    "\n",
    "            # save match_data to csv every loop\n",
    "            match_data.to_csv(f'{csv_path}', index=False)\n",
    "\n",
    "\n",
    "    return match_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'match_4699692683.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\smcgb\\OneDrive\\Desktop\\ld2l-data\\ld2lprototype.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m create_season_dataframe(\u001b[39m*\u001b[39;49mget_match_data(\u001b[39m*\u001b[39;49mget_od_matches(\u001b[39m*\u001b[39;49mget_ld2l_matches(\u001b[39m13\u001b[39;49m))))\n",
      "\u001b[1;32mc:\\Users\\smcgb\\OneDrive\\Desktop\\ld2l-data\\ld2lprototype.ipynb Cell 16\u001b[0m in \u001b[0;36mcreate_season_dataframe\u001b[1;34m(file_names, save_dir)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m# read json files in save_dir and append to match_data\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, file \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(file_names):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m     \u001b[39m# read json file\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m         match \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/smcgb/OneDrive/Desktop/ld2l-data/ld2lprototype.ipynb#Y106sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m     \u001b[39m# get match id\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'match_4699692683.json'"
     ]
    }
   ],
   "source": [
    "create_season_dataframe(*get_match_data(*get_od_matches(*get_ld2l_matches(13))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04cd323bcfca6d5122a158529f0e7b9beb1ef8bc4a464a63fcc23a16e9c333c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
