{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import bs4\n",
    "import time\n",
    "import os\n",
    "from config import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display full dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set ld2l season webpage\n",
    "url = 'https://ld2l.gg/seasons/37/matches'\n",
    "\n",
    "soup = bs4.BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "matches = []\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    if 'match' in a['href'] and 'season' not in a['href']:\n",
    "        matches.append('https://ld2l.gg' + a['href'])\n",
    "\n",
    "#sort matches by ID\n",
    "matches.sort(key=lambda x: int(x.split('/')[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matches text file to store match IDs if it doesn't exist\n",
    "if not os.path.exists('matches.txt'):\n",
    "    with open('matches.txt', 'w') as f:\n",
    "        f.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code is for getting opendota links\n",
    "\n",
    "od_matches = []\n",
    "\n",
    "for match in matches:\n",
    "    #check if match is already in file matches.txt to prevent re-scraping and angry butterygreg\n",
    "    if match in open('matches.txt').read():\n",
    "        pass\n",
    "    else:\n",
    "        with open('matches.txt', 'a') as f:\n",
    "            f.write(match + '\\n')\n",
    "        soup = bs4.BeautifulSoup(requests.get(match).text, 'html.parser')\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            if 'opendota' in a['href']:\n",
    "                if 'matches/0' in a['href']:\n",
    "                    break\n",
    "            # get match id from end of url\n",
    "                match_id = a['href'].split('/')[-1]\n",
    "                od_matches.append(f\"https://api.opendota.com/api/matches/{match_id}?api_key={api_key}\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold list of file names\n",
    "file_names = []\n",
    "\n",
    "for files in os.listdir():\n",
    "    if files.endswith('.json'):\n",
    "        file_names.append(files)\n",
    "\n",
    "for i, match in enumerate(od_matches):\n",
    "\n",
    "    # get match id\n",
    "    match_id = match.split('/')[-1].split('?')[0]\n",
    "\n",
    "    #check if file already exists\n",
    "    if os.path.isfile('match_' + match_id + '.json'):\n",
    "        pass\n",
    "    else:\n",
    "        if match_id == '0':\n",
    "            pass\n",
    "        # get json of match and save to json file\n",
    "        else:\n",
    "            match_json = requests.get(match).json()\n",
    "            with open('match_' + match_id + '.json', 'w') as f:\n",
    "                json.dump(match_json, f)\n",
    "                file_names.append('match_' + match_id + '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to hold all match data if it doesn't exist\n",
    "\n",
    "if not os.path.exists('match_data.csv'):\n",
    "    match_data = pd.DataFrame(columns=['match_id', 'date', 'week', 'account_id', 'personaname', 'teamID', 'rank_tier', 'kills', 'assists',\n",
    "       'deaths', 'kills_per_min', 'kda', 'denies', 'gold', 'gold_per_min', 'gold_spent', 'hero_damage', 'damage_taken',\n",
    "       'hero_healing', 'hero_id', 'item_0', 'item_1', 'item_2', 'item_3',\n",
    "       'item_4', 'item_5', 'item_neutral', 'last_hits', 'level',\n",
    "       'net_worth', 'tower_damage', 'xp_per_min', 'radiant_win',\n",
    "       'duration', 'patch', 'isRadiant', 'win', 'lose',\n",
    "       'total_gold', 'total_xp', 'obs_placed', 'sen_placed', 'rune_pickups', \n",
    "       'firstblood_claimed', 'pings', 'teamfight_participation', 'roshans_killed'])\n",
    "    match_data.to_csv('match_data.csv')\n",
    "else:\n",
    "    match_data = pd.read_csv('match_data.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  i, file in enumerate(file_names):\n",
    "\n",
    "    # read first json file as a dictionary\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # get match id\n",
    "    match_id = data['match_id']\n",
    "\n",
    "    # if match id is already in matches_df, skip\n",
    "    if match_id in match_data['match_id'].values:\n",
    "        pass\n",
    "    else:\n",
    "\n",
    "        rad_team_id = data['radiant_team_id']\n",
    "        dire_team_id = data['dire_team_id']\n",
    "        \n",
    "    # read player from data into a dataframe\n",
    "\n",
    "        df = pd.DataFrame(data['players'])\n",
    "\n",
    "        # damage taken needs to be transformed. it is a nested dictionary and should be replaced with the sum of the values\n",
    "\n",
    "        df['damage_taken'] = df['damage_taken'].apply(lambda x: sum(x.values()))\n",
    "\n",
    "        #convert start_time from unix time to datetime using\n",
    "        df['start_time'] = pd.to_datetime(df['start_time'], unit='s')\n",
    "        df['date'] = df['start_time'].dt.date\n",
    "\n",
    "        #games are played weekly. create a column for the week of the game. Week 1 starts on 2023-01-22, using isocalendar\n",
    "        df['week'] = df['start_time'].dt.isocalendar().week - 2\n",
    "\n",
    "        #drop start_time\n",
    "        df.drop('start_time', axis=1, inplace=True)\n",
    "\n",
    "        # if isRadiant is true, set teamID to radiant team ID, else set to dire team ID\n",
    "\n",
    "        df['teamID'] = df['isRadiant'].apply(lambda x: rad_team_id if x == True else dire_team_id)\n",
    "\n",
    "        new_order = ['match_id', 'date', 'week', 'account_id', 'personaname', 'teamID', 'rank_tier', 'kills', 'assists',\n",
    "       'deaths', 'kills_per_min', 'kda', 'denies', 'gold', 'gold_per_min', 'gold_spent', 'hero_damage', 'damage_taken',\n",
    "       'hero_healing', 'hero_id', 'item_0', 'item_1', 'item_2', 'item_3',\n",
    "       'item_4', 'item_5', 'item_neutral', 'last_hits', 'level',\n",
    "       'net_worth', 'tower_damage', 'xp_per_min', 'radiant_win',\n",
    "       'duration', 'patch', 'isRadiant', 'win', 'lose',\n",
    "       'total_gold', 'total_xp', 'obs_placed', 'sen_placed', 'rune_pickups', \n",
    "       'firstblood_claimed', 'pings', 'teamfight_participation', 'roshans_killed']\n",
    "\n",
    "        df = df[new_order]\n",
    "\n",
    "    # append to via concat\n",
    "\n",
    "        match_data = pd.concat([match_data, df], axis=0)\n",
    "\n",
    "        # save to csv every loop\n",
    "        match_data.to_csv('match_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04cd323bcfca6d5122a158529f0e7b9beb1ef8bc4a464a63fcc23a16e9c333c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
